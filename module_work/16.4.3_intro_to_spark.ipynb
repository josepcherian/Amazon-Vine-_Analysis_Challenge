{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"16.4.3_intro_to_spark.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyUUDBduA3IqyZ+t61+w2a"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"SGlHtHiPNexf","outputId":"60d7bdab-c29a-43d5-fbc2-3873ec9030dc"},"source":["# Module 16.4.1 PySpark in Google Colab Notebooks\r\n","import os\r\n","# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\r\n","# For example:\r\n","# spark_version = 'spark-3.0.1'\r\n","spark_version = 'spark-3.0.1'\r\n","os.environ['SPARK_VERSION']=spark_version\r\n","\r\n","# Install Spark and Java\r\n","!apt-get update\r\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\r\n","!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\r\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\r\n","!pip install -q findspark\r\n","\r\n","# Set Environment Variables\r\n","import os\r\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\r\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\r\n","\r\n","# Start a SparkSession\r\n","import findspark\r\n","findspark.init()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n","Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,725 kB]\n","Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [883 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,352 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,158 kB]\n","Ign:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n","Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [552 kB]\n","Fetched 7,942 kB in 3s (3,058 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.1-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"],"name":"stdout"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7a19ca332b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Start a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         raise Exception(\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"zQdS01sK4fSC"},"source":["# Module 16.4.2 Spark DataFrames and Datasets\r\n","# Start Spark session\r\n","from pyspark.sql import SparkSession\r\n","spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"CULyfi_P_6v8"},"source":["dataframe = spark.createDataFrame([\r\n","                                   (0,\"Here is our DataFrame\"),\r\n","                                   (1, \"We are making one from scratch\"),\r\n","                                   (2,\"This will look very similar to a Panda DataFrame\")\r\n","], [\"id\",\"words\"])\r\n","\r\n","dataframe.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"24Iqw_Fh_6y1"},"source":["# Read in data from S3 Buckets\r\n","from pyspark import SparkFiles\r\n","url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\r\n","spark.sparkContext.addFile(url)\r\n","df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"Ge7SxLbI_61a"},"source":["# Show DataFrame\r\n","df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"wYjRLEh2_7Sm"},"source":["# Print our schema\r\n","df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"f_DnUgNF_7VM"},"source":["# sShow the columns\r\n","df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"y507aNLCBdh0"},"source":["# Describe our data\r\n","df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"b1r8zooyBdj8"},"source":["# Import struct fields that we can use\r\n","from pyspark.sql.types import StructField, StringType, IntegerType, StructType"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"H6srIRasBdm9"},"source":["# Next we need to create the list of struct fields\r\n","schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\r\n","schema"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"vMVzgRJBBvpG"},"source":["# Pass in our fields\r\n","final = StructType(fields=schema)\r\n","final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"A-WhY47oBwiO"},"source":["# Read our data with our new schema\r\n","dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\r\n","dataframe.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"KSKjNWv1BwlD"},"source":["dataframe[\"price\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"T12-Otl7BwnY"},"source":["type(dataframe['price'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"eDrgAmk3B-3b"},"source":["dataframe.select('price')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"CzTEeY4zCCe6"},"source":["type(dataframe.select('price'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"SCtD-kd7CHnK"},"source":["dataframe.select('price').show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"UftsPmJ9CLji"},"source":["# Add new column\r\n","dataframe.withColumn('newprice', dataframe['price']).show()\r\n","# Update column name\r\n","dataframe.withColumnRenamed('price','newerprice').show()\r\n","# Double the price\r\n","dataframe.withColumn('doubleprice',dataframe['price']*2).show()\r\n","# Add a dollar to the price\r\n","dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()\r\n","# Half the price\r\n","dataframe.withColumn('half_price',dataframe['price']/2).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"OWH_j0aHCQBA"},"source":["# Module 16.4.3 Spark Functions\r\n","# Start Spark session\r\n","from pyspark.sql import SparkSession\r\n","spark = SparkSession.builder.appName(\"DataFrameFunctions\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"jrsegzsOCbhf"},"source":["# Read in data from S3 Buckets\r\n","from pyspark import SparkFiles\r\n","url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\r\n","spark.sparkContext.addFile(url)\r\n","df = spark.read.csv(SparkFiles.get(\"wine.csv\"), sep=\",\", header=True)\r\n","\r\n","# Show DataFrame\r\n","df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uDg9pfznCeMb"},"source":["# Order a DataFrame by ascending values\r\n","df.orderBy(df[\"points\"].desc())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"jxJmARx9CjwY"},"source":["df.orderBy(df[\"points\"].desc()).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"D7Gk4Fg3Cm4S"},"source":["#Skill Drill 16.4.3\r\n","df.orderBy(df[\"points\"].asc()).show(50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"z4Rg1zqfCm7C"},"source":["# Import Functions\r\n","from pyspark.sql.functions import avg \r\n","df.select(avg(\"points\")).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"mGXuKVczDS1q"},"source":["# Filter SQL method\r\n","df.filter(\"price<20\").show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"7s4uXoNjDS4E"},"source":["# Filter Python method\r\n","df.filter(\"price<20\").show(5)\r\n","# Filter by price on certain columns\r\n","df.filter(\"price<20\").select(['points','country', 'winery','price']).show(5)\r\n","\r\n","# Filter on exact state\r\n","df.filter(df[\"country\"] == \"US\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"sKXLrHcUCm-H"},"source":["# Skill Drill 16.4.3 \r\n","# Filter Python method\r\n","df.filter(\"price>15\").show(5)\r\n","# Filter by price on certain columns\r\n","df.filter(\"price>15\").select(['points','country', 'winery','price']).show(5)\r\n","\r\n","# Filter on exact state\r\n","df.filter(df[\"province\"] == \"California\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"esWKIOhPV4tf"},"source":["# Skill Drill 16.4.3 \r\n","# Filter Python method\r\n","df.filter(\"price>15\").show(5)\r\n","# Filter by price on certain columns\r\n","df.filter(\"price>15\").select(['points','country', 'winery','price']).show(5)\r\n","\r\n","# Filter on exact state\r\n","df.filter(df[\"province\"] == \"California\").show()"],"execution_count":null,"outputs":[]}]}